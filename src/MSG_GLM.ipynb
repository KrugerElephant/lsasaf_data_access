{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import thredds_lsasaf_utils as tlu\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod.families import Gaussian\n",
    "from statsmodels.genmod.families.links import Power\n",
    "from statsmodels.genmod.families import Gamma\n",
    "from statsmodels.genmod.families.links import log, identity\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import patsy\n",
    "\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounds and parameters\n",
    "lower_bound = 283.15  # Temperature in Kelvin (e.g., 0°C in Kelvin)\n",
    "upper_bound = 323.15  # Temperature in Kelvin (e.g., 50°C in Kelvin)\n",
    "no_data_percent = 0.18  # No data threshold\n",
    "sample_size = 2000  # Number of random rows to sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fused_data(filename):\n",
    "\n",
    "    # Replace 'your_file.csv' with the path to your CSV file\n",
    "    file_path = filename\n",
    "\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the CSV contains 'lat' and 'lon' columns\n",
    "    if 'lat' not in df.columns or 'lon' not in df.columns:\n",
    "        raise ValueError(\"The CSV file must contain 'lat' and 'lon' columns\")\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "\n",
    "    # Set the coordinate reference system (CRS) if known, e.g., WGS84 (EPSG:4326)\n",
    "    gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .csv file\n",
    "file_path = '../msg_lu_fused_data_for_glm/concatenated_data.gpkg'\n",
    "gdf =  gpd.read_file(file_path)  # Read the .gpkg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target and LULC columns\n",
    "target = 'temperature'  # Replace with your target column name\n",
    "\n",
    "# Convert to Kelvin\n",
    "gdf['temperature'] = gdf['temperature'] + 273.15\n",
    "\n",
    "# Ensure categorical variables are treated as such\n",
    "gdf['hour'] = gdf['hour'].astype('category')\n",
    "gdf['month'] = gdf['month'].astype('category')\n",
    "\n",
    "# Drop the geometry column\n",
    "gdf = gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Define LULC columns (all columns except the target)\n",
    "lulc_columns = ['water', 'trees', 'crop', 'built_area', 'bare_ground', 'range_land']\n",
    "\n",
    "# Drop rows with missing values\n",
    "gdf = gdf.dropna(subset=[target, 'hour', 'month'] + lulc_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Filter out rows outside this range\n",
    "gdf = gdf[(gdf[target] >= lower_bound) & (gdf[target] <= upper_bound)]\n",
    "\n",
    "# Drops rows with no_data > no_data_percent\n",
    "gdf = gdf[gdf['no_data'] <= no_data_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is called 'gdf' and the 'temperature' column exists\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting the histogram for temperature\n",
    "plt.hist(gdf['temperature'], bins=30, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Temperature Distribution')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_formula = f\"{target} ~ C(month) + C(hour) * ({' + '.join(lulc_columns)})\"\n",
    "interaction_formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Split the data into training and testing sets (0.50-0.50 split)\n",
    "train_data, test_data = train_test_split(gdf, test_size=0.70)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Step 3: Generate design matrices for train and test sets\n",
    "y_train, X_train = patsy.dmatrices(interaction_formula, data=train_data, return_type='dataframe')\n",
    "y_test, X_test = patsy.dmatrices(interaction_formula, data=test_data, return_type='dataframe')\n",
    "\n",
    "# Step 4: Fit the GLM model on the training data\n",
    "gamma_model = sm.GLM(y_train, X_train, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "gamma_results = gamma_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Display model summary\n",
    "print(gamma_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = gamma_results.predict(X_test)\n",
    "\n",
    "# Plot observed vs predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, edgecolors='k', label='Data points')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', label='Perfect Fit')\n",
    "plt.xlabel(\"Observed Temperature\")\n",
    "plt.ylabel(\"Predicted Temperature\")\n",
    "plt.title(\"Observed vs Predicted Temperature\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the GLM model to a pickle file\n",
    "with open('glm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(gamma_results, file)\n",
    "\n",
    "print(\"GLM model saved as 'glm_model.pkl'\")\n",
    "\n",
    "# Save the model summary to a text file\n",
    "with open('glm_model_summary.txt', 'w') as file:\n",
    "    file.write(gamma_results.summary().as_text())\n",
    "\n",
    "print(\"GLM model summary saved as 'glm_model_summary.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a combined 'month-hour' column for better visualization\n",
    "test_data['month_str'] = test_data['month'].astype(str).str.zfill(2)\n",
    "\n",
    "test_data['month_hour'] = test_data['month_str'].astype(str) + '-' + test_data['hour'].astype(str)\n",
    "\n",
    "# Add predictions to the test dataset\n",
    "test_data['observed'] = y_test.values\n",
    "test_data['predicted'] = y_pred\n",
    "\n",
    "# Sort by the combined 'month-hour' index for proper plotting\n",
    "test_data = test_data.sort_values(by=['month_str', 'hour'])\n",
    "\n",
    "# Plot observed vs predicted as a time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(test_data['month_hour'], test_data['observed'], label='Observed Temperature', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(test_data['month_hour'], test_data['predicted'], label='Predicted Temperature', marker='x', linestyle='--', color='red')\n",
    "plt.xlabel('Month-Hour')\n",
    "plt.ylabel('Temperature')\n",
    "plt.title('Time Series: Observed vs Predicted Temperature')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", round(rmse, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import patsy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Directory containing the Ecostress files\n",
    "ecostress_dir = \"../ecostress_lulc_temprature_datetime_fused/\"\n",
    "file_extension = \".gpkg\"\n",
    "\n",
    "# Land use columns and target column\n",
    "land_use_columns = ['water', 'trees', 'flooded_veg', 'crop', 'built_area', 'bare_ground', 'snow_or_ice', 'clouds', 'range_land']\n",
    "columns_to_check = land_use_columns + ['no_data']\n",
    "target = 'temperature'\n",
    "\n",
    "# Placeholder for RMSE calculation and combined plotting\n",
    "all_observed = []\n",
    "all_predicted = []\n",
    "\n",
    "# Extract only the predictors part of the formula\n",
    "predictor_formula = interaction_formula.split(\"~\")[1].strip()\n",
    "\n",
    "# Iterate over all Ecostress files\n",
    "for file_name in os.listdir(ecostress_dir):\n",
    "    if file_name.endswith(file_extension):\n",
    "        ecostress_file = os.path.join(ecostress_dir, file_name)\n",
    "\n",
    "        print(file_name)\n",
    "        # Read the file\n",
    "        new_gdf = gpd.read_file(ecostress_file)\n",
    "\n",
    "        # Filter for summer months\n",
    "        new_gdf = new_gdf[new_gdf['month'].isin([6, 7, 8])]\n",
    "\n",
    "        # Filter rows based on land use and no_data columns\n",
    "        new_gdf = new_gdf[~(new_gdf[columns_to_check] == 0).all(axis=1)]\n",
    "        new_gdf = new_gdf[new_gdf['no_data'] <= no_data_percent]\n",
    "\n",
    "        # Convert temperature to Kelvin\n",
    "        new_gdf['temperature'] = new_gdf['temperature'] + 273.15\n",
    "\n",
    "        # Filter rows within the temperature bounds\n",
    "        new_gdf = new_gdf[(new_gdf[target] >= lower_bound) & (new_gdf[target] <= upper_bound)]\n",
    "\n",
    "        # Ensure categorical variables\n",
    "        new_gdf['hour'] = new_gdf['hour'].astype('category')\n",
    "        new_gdf['month'] = new_gdf['month'].astype('category')\n",
    "\n",
    "        # Ensure consistent categories\n",
    "        new_gdf['month'] = pd.Categorical(new_gdf['month'], categories=gdf['month'].unique())\n",
    "        new_gdf['hour'] = pd.Categorical(new_gdf['hour'], categories=gdf['hour'].unique())\n",
    "\n",
    "        # Drop rows with missing values\n",
    "        new_gdf = new_gdf.dropna(subset=['temperature', 'hour', 'month'])\n",
    "\n",
    "        # Randomly sample 2000 rows\n",
    "        if len(new_gdf) > sample_size:\n",
    "            new_gdf = new_gdf.sample(n=sample_size)\n",
    "\n",
    "        if new_gdf.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Generate design matrix\n",
    "        X_new = patsy.dmatrix(predictor_formula, data=new_gdf, return_type='dataframe')\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = gamma_results.predict(X_new)\n",
    "\n",
    "        # Cap predictions at bounds\n",
    "        capped_predictions = np.minimum(np.maximum(predictions, lower_bound), upper_bound)\n",
    "        new_gdf['predicted'] = capped_predictions\n",
    "\n",
    "        # Store observed and predicted values for combined plot\n",
    "        all_observed.extend(new_gdf['temperature'].tolist())\n",
    "        all_predicted.extend(new_gdf['predicted'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot all observed vs. predicted values in one plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(all_observed, all_predicted, alpha=0.7, edgecolor='k', label='Data Points')\n",
    "\n",
    "min_val = lower_bound\n",
    "max_val = upper_bound\n",
    "\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', linewidth=1, label='Perfect Fit')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Observed Temperature')\n",
    "plt.ylabel('Predicted Temperature')\n",
    "plt.title('Combined Observed vs Predicted Temperatures')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate and print overall RMSE\n",
    "rmse = np.sqrt(mean_squared_error(all_observed, all_predicted))\n",
    "print(f\"Overall RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "import numpy as np\n",
    "\n",
    "def calculate_rmsle(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Root Mean Squared Logarithmic Error (RMSLE) using scikit-learn.\n",
    "\n",
    "    Parameters:\n",
    "        actual (array-like): The actual observed values.\n",
    "        predicted (array-like): The predicted values.\n",
    "\n",
    "    Returns:\n",
    "        float: The RMSLE value.\n",
    "    \"\"\"\n",
    "    # Ensure the values are positive (to avoid log issues)\n",
    "    actual = np.maximum(0, actual)  # Negative values are clipped to 0\n",
    "    predicted = np.maximum(0, predicted)  # Negative values are clipped to 0\n",
    "\n",
    "    # Calculate Mean Squared Logarithmic Error\n",
    "    msle = mean_squared_log_error(actual, predicted)\n",
    "    rmsle = np.sqrt(msle)\n",
    "    return rmsle\n",
    "\n",
    "\n",
    "rmsle_value = calculate_rmsle(all_observed, all_predicted)\n",
    "print(f\"RMSLE: {rmsle_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_formula"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

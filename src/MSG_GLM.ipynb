{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import thredds_lsasaf_utils as tlu\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.genmod.families import Gaussian\n",
    "from statsmodels.genmod.families.links import Power\n",
    "from statsmodels.genmod.families import Gamma\n",
    "from statsmodels.genmod.families.links import log, identity\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import patsy\n",
    "\n",
    "import common_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fused_data(filename):\n",
    "\n",
    "    # Replace 'your_file.csv' with the path to your CSV file\n",
    "    file_path = filename\n",
    "\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Ensure the CSV contains 'lat' and 'lon' columns\n",
    "    if 'lat' not in df.columns or 'lon' not in df.columns:\n",
    "        raise ValueError(\"The CSV file must contain 'lat' and 'lon' columns\")\n",
    "\n",
    "    # Create a GeoDataFrame\n",
    "    geometry = [Point(xy) for xy in zip(df['lon'], df['lat'])]\n",
    "    gdf = gpd.GeoDataFrame(df, geometry=geometry)\n",
    "\n",
    "    # Set the coordinate reference system (CRS) if known, e.g., WGS84 (EPSG:4326)\n",
    "    gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from .csv file\n",
    "gdf = read_fused_data('../msg_lu_fused_data_for_glm/lst_data._bkup.csv')\n",
    "# file_path = '../msg_lu_fused_data_for_glm/msg_lu_fused_data_for_glm.gpkg'\n",
    "# gdf =  gpd.read_file(file_path)  # Read the .gpkg file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define target and LULC columns\n",
    "target = 'temperature'  # Replace with your target column name\n",
    "\n",
    "# Convert to Kelvin\n",
    "gdf['temperature'] = gdf['temperature'] + 273.15\n",
    "\n",
    "# Ensure categorical variables are treated as such\n",
    "gdf['hour'] = gdf['hour'].astype('category')\n",
    "gdf['month'] = gdf['month'].astype('category')\n",
    "\n",
    "# Drop the geometry column\n",
    "gdf = gdf.drop(columns=['geometry'])\n",
    "\n",
    "# Define LULC columns (all columns except the target)\n",
    "lulc_columns = ['water', 'trees', 'crop', 'built_area', 'bare_ground', 'range_land']\n",
    "\n",
    "# Drop rows with missing values\n",
    "gdf = gdf.dropna(subset=[target, 'hour', 'month'] + lulc_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the 1st and 99th percentiles for the target\n",
    "lower_bound = np.percentile(gdf[target], 1)\n",
    "upper_bound = np.percentile(gdf[target], 99)\n",
    "\n",
    "# Step 2: Filter out rows outside this range\n",
    "gdf = gdf[(gdf[target] >= lower_bound) & (gdf[target] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame is called 'gdf' and the 'temperature' column exists\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting the histogram for temperature\n",
    "plt.hist(gdf['temperature'], bins=30, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Temperature Distribution')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "interaction_formula = f\"{target} ~ C(month) + C(hour) * ({' + '.join(lulc_columns)})\"\n",
    "interaction_formula\n",
    "\n",
    "# Step 2: Split the data into training and testing sets (0.50-0.50 split)\n",
    "train_data, test_data = train_test_split(gdf, test_size=0.40)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Step 3: Generate design matrices for train and test sets\n",
    "y_train, X_train = patsy.dmatrices(interaction_formula, data=train_data, return_type='dataframe')\n",
    "y_test, X_test = patsy.dmatrices(interaction_formula, data=test_data, return_type='dataframe')\n",
    "\n",
    "# Step 4: Fit the GLM model on the training data\n",
    "gamma_model = sm.GLM(y_train, X_train, family=sm.families.Gamma(link=sm.families.links.log()))\n",
    "gamma_results = gamma_model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Display model summary\n",
    "print(gamma_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "y_pred = gamma_results.predict(X_test)\n",
    "\n",
    "# Plot observed vs predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, edgecolors='k', label='Data points')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', label='Perfect Fit')\n",
    "plt.xlabel(\"Observed Temperature\")\n",
    "plt.ylabel(\"Predicted Temperature\")\n",
    "plt.title(\"Observed vs Predicted Temperature\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the GLM model to a pickle file\n",
    "with open('glm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(gamma_results, file)\n",
    "\n",
    "print(\"GLM model saved as 'glm_model.pkl'\")\n",
    "\n",
    "# Save the model summary to a text file\n",
    "with open('glm_model_summary.txt', 'w') as file:\n",
    "    file.write(gamma_results.summary().as_text())\n",
    "\n",
    "print(\"GLM model summary saved as 'glm_model_summary.txt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a combined 'month-hour' column for better visualization\n",
    "test_data['month_str'] = test_data['month'].astype(str).str.zfill(2)\n",
    "\n",
    "test_data['month_hour'] = test_data['month_str'].astype(str) + '-' + test_data['hour'].astype(str)\n",
    "\n",
    "# Add predictions to the test dataset\n",
    "test_data['observed'] = y_test.values\n",
    "test_data['predicted'] = y_pred\n",
    "\n",
    "# Sort by the combined 'month-hour' index for proper plotting\n",
    "test_data = test_data.sort_values(by=['month_str', 'hour'])\n",
    "\n",
    "# Plot observed vs predicted as a time series\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(test_data['month_hour'], test_data['observed'], label='Observed Temperature', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(test_data['month_hour'], test_data['predicted'], label='Predicted Temperature', marker='x', linestyle='--', color='red')\n",
    "plt.xlabel('Month-Hour')\n",
    "plt.ylabel('Temperature')\n",
    "plt.title('Time Series: Observed vs Predicted Temperature')\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "print(\"Root Mean Squared Error:\", round(rmse, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "concatenated_file = os.path.join('../ecostress_lulc_fused_data_for_glm', 'concatenated_output.gpkg')\n",
    "\n",
    "# Read the concatenated file\n",
    "new_gdf = gpd.read_file(concatenated_file)  # Read the .gpkg file\n",
    "\n",
    "new_gdf = new_gdf[new_gdf['month'].isin([6, 7, 8])]\n",
    "\n",
    "new_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Kelvin\n",
    "new_gdf['temperature'] = new_gdf['temperature'] + 273.15\n",
    "\n",
    "# Ensure categorical variables are treated as such\n",
    "new_gdf['hour'] = new_gdf['hour'].astype('category')\n",
    "new_gdf['month'] = new_gdf['month'].astype('category')\n",
    "\n",
    "# Drop rows with missing values\n",
    "new_gdf = new_gdf.dropna(subset=['temperature', 'hour', 'month'])\n",
    "new_gdf.shape\n",
    "new_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'datetime' column exists or create one from year, month, day, hour\n",
    "if 'time' not in new_gdf.columns:\n",
    "    new_gdf['time'] = pd.to_datetime(\n",
    "        new_gdf[['year', 'month', 'day', 'hour']].astype(str).agg('-'.join, axis=1)\n",
    "    )\n",
    "\n",
    "# Sort the data by datetime\n",
    "new_gdf = new_gdf.sort_values(by='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the first few days and explicitly create a copy\n",
    "first_few_days = new_gdf[new_gdf['time'] < (new_gdf['time'].min() + pd.Timedelta(days=1))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_few_days.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the categories of the categorical variables\n",
    "print(\"Categories in training data:\")\n",
    "print(f\"Month: {gdf['month'].unique()}\")\n",
    "print(f\"Hour: {gdf['hour'].unique()}\")\n",
    "\n",
    "print(\"\\nCategories in prediction data:\")\n",
    "print(f\"Month: {first_few_days['month'].unique()}\")\n",
    "print(f\"Hour: {first_few_days['hour'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure consistent categories\n",
    "first_few_days['month'] = pd.Categorical(first_few_days['month'], categories=gdf['month'].unique())\n",
    "first_few_days['hour'] = pd.Categorical(first_few_days['hour'], categories=gdf['hour'].unique())\n",
    "\n",
    "# # Recreate the design matrix for the new data\n",
    "# _, X_test = patsy.dmatrices(interaction_formula, data=first_few_days, return_type='dataframe')\n",
    "\n",
    "# # Check the design matrix\n",
    "# print(f\"X_test shape: {X_test.shape}\")\n",
    "# print(f\"Model parameters shape: {gamma_results.params.shape}\")\n",
    "\n",
    "# # Inspect column names in X_test vs X_train\n",
    "# print(\"X_test columns:\", X_test.columns)\n",
    "\n",
    "# Predict using the trained model\n",
    "# first_few_days['predicted'] = gamma_results.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the predictors part of the formula\n",
    "predictor_formula = interaction_formula.split(\"~\")[1].strip()\n",
    "print(predictor_formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Generate the design matrix for new data\n",
    "X_new = patsy.dmatrix(predictor_formula, data=first_few_days, return_type='dataframe')\n",
    "\n",
    "# Make predictions using the trained model\n",
    "# predictions = gamma_results.predict(X_new)\n",
    "first_few_days['predicted'] = gamma_results.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(first_few_days['temperature'], first_few_days['predicted'], alpha=0.7, edgecolor='k')\n",
    "# Scatter plot\n",
    "\n",
    "# Add a reference line for perfect predictions\n",
    "max_val = max(first_few_days['temperature'].max(), first_few_days['predicted'].max())\n",
    "min_val = min(first_few_days['temperature'].min(), first_few_days['predicted'].min())\n",
    "\n",
    "plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--', linewidth=1, label='Perfect Fit')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Observed Temperature')\n",
    "plt.ylabel('Predicted Temperature')\n",
    "plt.title('Observed vs Predicted Temperatures')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Replace 'temperature' and 'predicted_temperature' with actual column names\n",
    "observed = first_few_days['temperature']\n",
    "predicted = first_few_days['predicted']\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(observed, predicted))\n",
    "\n",
    "print(f\"RMSE: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where the observed and predicted values differ by more than 5:\n",
      "         no_data  water     trees  flooded_veg      crop  built_area  \\\n",
      "4002269      0.0    0.0  0.000000          0.0  0.000000    1.000000   \n",
      "4021066      0.0    0.0  0.000000          0.0  0.015873    0.984127   \n",
      "4021065      0.0    0.0  0.000000          0.0  0.000000    1.000000   \n",
      "4021064      0.0    0.0  0.300000          0.0  0.700000    0.000000   \n",
      "4021062      0.0    0.0  0.938462          0.0  0.000000    0.000000   \n",
      "...          ...    ...       ...          ...       ...         ...   \n",
      "864342       0.0    0.0  0.938462          0.0  0.000000    0.000000   \n",
      "864344       0.0    0.0  0.300000          0.0  0.700000    0.000000   \n",
      "864345       0.0    0.0  0.000000          0.0  0.000000    1.000000   \n",
      "864334       0.0    0.0  0.000000          0.0  0.000000    0.000000   \n",
      "901919       0.0    0.0  0.000000          0.0  1.000000    0.000000   \n",
      "\n",
      "         bare_ground  snow_or_ice  clouds  range_land  year month  day hour  \\\n",
      "4002269          0.0          0.0     0.0    0.000000  2024     6    4   10   \n",
      "4021066          0.0          0.0     0.0    0.000000  2024     6    4   10   \n",
      "4021065          0.0          0.0     0.0    0.000000  2024     6    4   10   \n",
      "4021064          0.0          0.0     0.0    0.000000  2024     6    4   10   \n",
      "4021062          0.0          0.0     0.0    0.061538  2024     6    4   10   \n",
      "...              ...          ...     ...         ...   ...   ...  ...  ...   \n",
      "864342           0.0          0.0     0.0    0.061538  2024     6    5    9   \n",
      "864344           0.0          0.0     0.0    0.000000  2024     6    5    9   \n",
      "864345           0.0          0.0     0.0    0.000000  2024     6    5    9   \n",
      "864334           0.0          0.0     0.0    1.000000  2024     6    5    9   \n",
      "901919           0.0          0.0     0.0    0.000000  2024     6    5    9   \n",
      "\n",
      "         temperature                   geometry                time  \\\n",
      "4002269       298.58  POINT (12.47376 41.78981) 2024-06-04 10:00:00   \n",
      "4021066       276.22  POINT (12.48896 42.04021) 2024-06-04 10:00:00   \n",
      "4021065       288.46  POINT (12.50096 41.77861) 2024-06-04 10:00:00   \n",
      "4021064       282.12  POINT (12.46736 41.96501) 2024-06-04 10:00:00   \n",
      "4021062       287.84  POINT (12.29536 41.86261) 2024-06-04 10:00:00   \n",
      "...              ...                        ...                 ...   \n",
      "864342        287.84  POINT (12.29536 41.86261) 2024-06-05 09:00:00   \n",
      "864344        282.12  POINT (12.46736 41.96501) 2024-06-05 09:00:00   \n",
      "864345        288.46  POINT (12.50096 41.77861) 2024-06-05 09:00:00   \n",
      "864334        301.16  POINT (12.42016 41.77461) 2024-06-05 09:00:00   \n",
      "901919        275.92  POINT (12.44656 42.04581) 2024-06-05 09:00:00   \n",
      "\n",
      "          predicted  \n",
      "4002269  309.303243  \n",
      "4021066  309.345381  \n",
      "4021065  309.303243  \n",
      "4021064  308.335108  \n",
      "4021062  300.699382  \n",
      "...             ...  \n",
      "864342   298.693983  \n",
      "864344   306.124760  \n",
      "864345   305.832572  \n",
      "864334   311.920965  \n",
      "901919   309.742651  \n",
      "\n",
      "[208981 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "difference = np.abs(observed - predicted)\n",
    "\n",
    "# Step 3: Identify rows where the difference exceeds 5\n",
    "large_diff_indices = np.where(difference > 5)[0]\n",
    "\n",
    "# Step 4: Extract corresponding rows from the test data\n",
    "rows_with_large_difference = first_few_days.iloc[large_diff_indices]\n",
    "\n",
    "# Print the results\n",
    "print(\"Rows where the observed and predicted values differ by more than 5:\")\n",
    "print(rows_with_large_difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225480, 18)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_few_days.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208981"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rows_with_large_difference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
